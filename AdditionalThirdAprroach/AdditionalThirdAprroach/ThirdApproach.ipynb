{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6wyeDYOx-qZ"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract transformers torch scikit-learn\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y libtesseract-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the folder path and batch size"
      ],
      "metadata": {
        "id": "eLgrE4Xd0lwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Slh5mzQxwzt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from transformers import BertTokenizerFast, BertModel, LayoutLMTokenizer, LayoutLMModel\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from google.colab import drive\n",
        "import logging\n",
        "import math\n",
        "\n",
        "# Suppress transformers logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21ej8etZypEt"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up paths\n",
        "folder_path = '/content/drive/My Drive/resume-processed'  # Adjust the path to your folder in Google Drive\n",
        "output_folder_path = '/content/drive/My Drive/resume-clusters_bert-layoutlm'\n",
        "file_limit = 2500  # Set the limit for the number of files to process\n",
        "batch_size = 1  # Increase batch size to better utilize 40GB GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking if GPU device is available and use it\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JCqSWeFa0rPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for CUDA device and set device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "uXiIrbia0wne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining function to extract features from the images processed by denoiser"
      ],
      "metadata": {
        "id": "flgd0pO_0z9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define a function `extract_text_from_image` that utilizes Optical Character Recognition (OCR) to extract text from an image file.\n",
        "The function takes an image file path as input, resizes the image to a standard width while maintaining the aspect ratio, and then applies OCR using the Tesseract library to extract and return the text content from the image.\n",
        "\n",
        "The steps involved in the function are:\n",
        "1. Open the image file.\n",
        "2. Resize the image to a base width of 1000 pixels using the LANCZOS filter for high-quality downsampling.\n",
        "3. Apply Tesseract OCR to the resized image to extract text.\n",
        "4. Return the extracted text.\n",
        "\n",
        "This preprocessing step ensures that the text in the images is readable and standardized for further analysis."
      ],
      "metadata": {
        "id": "82s3RZ2E06G1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPMN-8UZyyOH"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_image(image_path):\n",
        "    # Open the image file\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Resizing the image to a base width of 1000 pixels while maintaining aspect ratio\n",
        "    base_width = 1000\n",
        "    w_percent = (base_width / float(img.size[0]))  # Calculate the width percentage\n",
        "    h_size = int((float(img.size[1]) * float(w_percent)))  # Calculate the new height based on the width percentage\n",
        "    img = img.resize((base_width, h_size), Image.LANCZOS)  # Resize the image using LANCZOS filter for high-quality downsampling\n",
        "\n",
        "    # Perform OCR on the resized image to extract text\n",
        "    text = pytesseract.image_to_string(img)\n",
        "\n",
        "    # Return the extracted text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Text Features Using BERT\n",
        "\n",
        "In this section, we define a function `extract_text_features` that utilizes a pre-trained BERT model to extract feature embeddings from a given text. The function takes the text, a tokenizer, and a model as input, tokenizes the text, processes it through the BERT model, and returns the feature embeddings.\n",
        "\n",
        "The steps involved in the function are:\n",
        "1. **Tokenize the Text**: The text is tokenized using the provided tokenizer with truncation and padding to ensure the input length is consistent and within the model's limits.\n",
        "2. **Generate Input Tensors**: The tokenized inputs are converted to PyTorch tensors and moved to the appropriate device (CPU or GPU).\n",
        "3. **Model Inference**: The BERT model processes the input tensors to generate output embeddings, with computations done in a `torch.no_grad()` context to avoid gradient calculation.\n",
        "4. **Extract Features**: The function extracts the mean of the last hidden state embeddings from the model output, converts them to a numpy array, and returns the features.\n",
        "\n",
        "This function is crucial for converting raw text into meaningful numerical representations (embeddings) that capture the semantic information of the text for further analysis.\n",
        "\n",
        "#### Technical Details and Model Choice\n",
        "\n",
        "**BERT (Bidirectional Encoder Representations from Transformers)**:\n",
        "- **Architecture**: BERT uses a transformer-based architecture, specifically the encoder part of the transformer. It consists of multiple layers (12 in the base version) of bidirectional self-attention mechanisms, which allow it to consider both left and right context simultaneously.\n",
        "- **Training**: BERT is pre-trained on a large corpus of text (e.g., Wikipedia, BookCorpus) using two unsupervised tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). This pre-training enables BERT to capture deep contextual representations of language.\n",
        "- **Tokenization**: BERT uses WordPiece tokenization, which breaks down words into subword units, allowing it to handle a large vocabulary efficiently and manage out-of-vocabulary words effectively.\n",
        "- **Fine-tuning**: BERT can be fine-tuned for specific tasks with relatively small amounts of labeled data, leveraging its pre-trained knowledge to achieve state-of-the-art performance in various NLP tasks.\n",
        "\n",
        "**Why BERT?**:\n",
        "1. **Contextual Understanding**: BERT's bidirectional attention mechanism allows it to understand the context of a word based on its surrounding words, providing rich, contextual embeddings.\n",
        "2. **Pre-trained Knowledge**: BERT's pre-training on vast amounts of text data makes it highly effective at capturing semantic nuances, even with limited labeled data for fine-tuning.\n",
        "3. **Versatility**: BERT can be applied to a wide range of NLP tasks, including text classification, named entity recognition, and question answering, making it a versatile choice for feature extraction.\n",
        "4. **State-of-the-art Performance**: BERT has consistently achieved state-of-the-art results on numerous NLP benchmarks, demonstrating its effectiveness in understanding and generating text representations.\n",
        "\n",
        "By using BERT for text feature extraction, we leverage its ability to generate rich, contextual embeddings that capture the semantic meaning of the text, providing a strong foundation for subsequent analysis.\n"
      ],
      "metadata": {
        "id": "3vup4Nh71CFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH0_4iWNyw-N"
      },
      "outputs": [],
      "source": [
        "def extract_text_features(text, tokenizer, model):\n",
        "    # Tokenize the text with truncation and padding, and convert to PyTorch tensors\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(device)\n",
        "\n",
        "    # Perform model inference without gradient calculation\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the mean of the last hidden state embeddings and convert to numpy array\n",
        "    features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Layout Features Using LayoutLM\n",
        "\n",
        "In this section, we define a function `extract_layout_features` that utilizes a pre-trained LayoutLM model to extract layout-aware feature embeddings from an image. The function takes an image file path, a tokenizer, and a model as input, processes the image to extract text and layout information, and returns the feature embeddings.\n",
        "\n",
        "The steps involved in the function are:\n",
        "1. **Open and Resize Image**: The image is opened and resized to a standard width while maintaining the aspect ratio to ensure text readability.\n",
        "2. **Extract OCR Data**: Tesseract OCR is used to extract text and layout information (bounding boxes) from the image.\n",
        "3. **Normalize Bounding Boxes**: The bounding boxes are normalized relative to the image dimensions.\n",
        "4. **Tokenize Text and Layout Information**: The extracted text and bounding boxes are tokenized using the LayoutLM tokenizer with truncation and padding.\n",
        "5. **Model Inference**: The LayoutLM model processes the input tensors to generate output embeddings, with computations done in a `torch.no_grad()` context.\n",
        "6. **Extract Features**: The function extracts the mean of the last hidden state embeddings from the model output, converts them to a numpy array, and returns the features.\n",
        "\n",
        "This function is crucial for capturing both textual and spatial information from document images, providing a comprehensive feature representation for further analysis.\n",
        "\n",
        "#### Technical Details and Model Choice\n",
        "\n",
        "**LayoutLM (Layout-Aware Language Model)**:\n",
        "- **Architecture**: LayoutLM extends the BERT architecture by incorporating layout information. It uses the same transformer-based architecture but adds an additional input embedding for the spatial layout (bounding boxes) of the text.\n",
        "- **Training**: LayoutLM is pre-trained on large-scale document datasets, learning to understand both text and its spatial arrangement. It uses tasks such as masked language modeling and structure-aware pre-training to capture the relationships between text and layout.\n",
        "- **Tokenization**: LayoutLM uses a tokenizer similar to BERT but additionally requires bounding box coordinates for each token. These coordinates help the model understand the spatial structure of the document.\n",
        "- **Fine-tuning**: LayoutLM can be fine-tuned for various document understanding tasks, such as form understanding, receipt parsing, and document classification, by leveraging its pre-trained knowledge of text and layout.\n",
        "\n",
        "**Why LayoutLM?**:\n",
        "1. **Text and Layout Integration**: LayoutLM captures both textual and spatial information, making it ideal for tasks where the layout of the text is crucial for understanding the document.\n",
        "2. **Pre-trained on Document Data**: LayoutLM is pre-trained on a large corpus of documents, allowing it to generalize well to various document types and structures.\n",
        "3. **Versatility**: LayoutLM can be fine-tuned for a wide range of document-related tasks, providing flexibility and robustness.\n",
        "4. **State-of-the-art Performance**: LayoutLM has achieved state-of-the-art results on several document understanding benchmarks, demonstrating its effectiveness in capturing the interplay between text and layout.\n",
        "\n",
        "By using LayoutLM for layout feature extraction, we leverage its ability to understand the spatial relationships between text elements, providing a comprehensive feature representation that includes both textual and layout information.\n"
      ],
      "metadata": {
        "id": "8In7gsC21QN2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQXs7q5eyu-n"
      },
      "outputs": [],
      "source": [
        "def extract_layout_features(image_path, tokenizer, model):\n",
        "    # Open the image and convert to RGB\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Resizing the image to a base width of 1000 pixels while maintaining aspect ratio\n",
        "    base_width = 1000\n",
        "    w_percent = (base_width / float(image.size[0]))  # Calculate the width percentage\n",
        "    h_size = int((float(image.size[1]) * float(w_percent)))  # Calculate the new height based on the width percentage\n",
        "    image = image.resize((base_width, h_size), Image.LANCZOS)  # Resize the image using LANCZOS filter for high-quality downsampling\n",
        "    width, height = image.size  # Get the new dimensions of the image\n",
        "\n",
        "    words, boxes, actual_boxes = [], [], []\n",
        "\n",
        "    # Use Tesseract to extract OCR data from the image\n",
        "    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
        "    n_boxes = len(data['level'])  # Get the number of detected text elements\n",
        "\n",
        "    for i in range(n_boxes):\n",
        "        (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])  # Get the bounding box coordinates\n",
        "        words.append(data['text'][i])  # Append the detected text\n",
        "        boxes.append([x, y, x + w, y + h])  # Append the bounding box coordinates\n",
        "        actual_boxes.append([x / width, y / height, (x + w) / width, (y + h) / height])  # Normalize the bounding boxes\n",
        "\n",
        "    # Tokenize the words and bounding boxes\n",
        "    encoded_inputs = tokenizer(\n",
        "        words,\n",
        "        boxes=actual_boxes,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512  # Increase max_length to handle longer texts\n",
        "    )\n",
        "    encoded_inputs = {key: tensor.to(device) for key, tensor in encoded_inputs.items()}  # Move tensors to the appropriate device\n",
        "\n",
        "    # Perform model inference without gradient calculation\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_inputs)\n",
        "\n",
        "    # Extract the mean of the last hidden state embeddings and convert to numpy array\n",
        "    features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Tokenizers and Models\n",
        "\n",
        "In this section, we initialize the tokenizers and models for both text and layout feature extraction using pre-trained BERT and LayoutLM models. These models are loaded from the Hugging Face model hub and moved to the appropriate device (CPU or GPU).\n",
        "\n",
        "The steps involved in this section are:\n",
        "1. **Initialize BERT Tokenizer and Model**: Load the pre-trained BERT tokenizer and model for text feature extraction.\n",
        "2. **Initialize LayoutLM Tokenizer and Model**: Load the pre-trained LayoutLM tokenizer and model for layout-aware feature extraction.\n",
        "3. **Move Models to Device**: Move the models to the appropriate device (CPU or GPU) to leverage hardware acceleration for faster processing.\n",
        "\n",
        "This setup ensures that we have the necessary tools for extracting both textual and spatial information from document images, providing a comprehensive feature representation for further analysis.\n"
      ],
      "metadata": {
        "id": "ihW6HQkH1Zzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PuqgNXs3T-A"
      },
      "outputs": [],
      "source": [
        "text_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "text_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "layout_tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
        "layout_model = LayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Features from All Images with Progress Tracking\n",
        "\n",
        "In this section, we extract text and layout features from all images in the specified folder with progress tracking. The function processes each image to extract text and layout information, calculates the total number of words for a progress bar, and prepares the data for further analysis.\n",
        "\n",
        "The steps involved in this section are:\n",
        "1. **Initialize Lists for Features**: Create empty lists to store extracted texts, text features, and layout features.\n",
        "2. **Print Extraction Message**: Inform the user that feature extraction is starting.\n",
        "3. **Collect File Paths**: Gather all file paths of `.tif` images in the specified folder.\n",
        "4. **Estimate Total Number of Words**: Calculate the total number of words to process for a progress bar:\n",
        "   - **Initialize Total Words Counter**: Set up a counter to track the total number of words.\n",
        "   - **Initialize Progress Bar**: Use `tqdm` to create a progress bar for tracking the estimation process.\n",
        "   - **Process Each Image**: For each image, perform OCR to extract text elements and count the total number of words.\n",
        "\n",
        "This setup ensures that we can monitor the progress of feature extraction and handle large datasets efficiently.\n"
      ],
      "metadata": {
        "id": "fn5JQX1I1fpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVC_jNB_DrqW"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store texts, text features, and layout features\n",
        "texts = []\n",
        "text_features = []\n",
        "layout_features = []\n",
        "\n",
        "\n",
        "print(\"Extracting features from images...\")\n",
        "\n",
        "# Initialize file count\n",
        "file_count = 0\n",
        "\n",
        "# Collect file paths for all .tif images in the specified folder\n",
        "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif')]\n",
        "\n",
        "# Estimate total number of words to process for the progress bar\n",
        "print(\"Estimating total number of words to process...\")\n",
        "total_words = 0\n",
        "\n",
        "# Initialize the progress bar for word estimation\n",
        "with tqdm(total=len(file_paths[:file_limit]), desc=\"Estimating words\", unit=\"file\", leave=False) as pbar:\n",
        "    for file_path in file_paths[:file_limit]:\n",
        "        # Open the image and convert to RGB\n",
        "        data = pytesseract.image_to_data(Image.open(file_path).convert(\"RGB\"), output_type=pytesseract.Output.DICT)\n",
        "\n",
        "        # Sum up the number of detected text elements for word count\n",
        "        total_words += len(data['text'])\n",
        "\n",
        "        # Update the progress bar\n",
        "        pbar.update(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process a Batch of Images\n",
        "\n",
        "In this section, we define a series of functions to process a batch of image files, extracting text, text features, and layout features. The functions leverage previously defined methods for OCR-based text extraction, text feature extraction using BERT, and layout feature extraction using LayoutLM. This modular approach allows for efficient and scalable batch processing, especially useful for large datasets.\n",
        "\n",
        "## Steps Involved\n",
        "\n",
        "1. **Extract Text from Each Image:** Using `process_batch_1`, the text is extracted from each image file in the batch using OCR.\n",
        "2. **Extract Text Features:** Using `process_batch_2`, text features are computed for each extracted text using the BERT model and tokenizer.\n",
        "3. **Extract Layout Features:** Using `process_batch_3`, layout features are computed for each image file in the batch using the LayoutLM model and tokenizer.\n",
        "4. **Clear Memory:** The `clear_memory` function is used to release GPU memory and perform garbage collection to maintain efficiency."
      ],
      "metadata": {
        "id": "zoySzvYS1kdH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFGWKg8YQSg-"
      },
      "outputs": [],
      "source": [
        "def process_batch_1(batch_files):\n",
        "    batch_texts = [extract_text_from_image(file_path) for file_path in batch_files]\n",
        "    return batch_texts\n",
        "\n",
        "def process_batch_2(batch_files, batch_texts):\n",
        "    batch_text_features = [extract_text_features(text, text_tokenizer, text_model) for text in batch_texts]\n",
        "    return batch_text_features\n",
        "\n",
        "def process_batch_3(batch_files):\n",
        "    batch_layout_features = [extract_layout_features(file_path, layout_tokenizer, layout_model) for file_path in batch_files]\n",
        "    return batch_layout_features\n",
        "\n",
        "def clear_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Files in Batches\n",
        "\n",
        "In this section, we process the files in batches to extract and store text, text features, and layout features from each image. A progress bar is used to track the tokenization process, providing a visual indication of the progress. Additionally, memory is managed by clearing the cache after each batch is processed. The process is limited to a specified number of files to ensure efficiency and manageability.\n",
        "\n",
        "## Steps Involved\n",
        "\n",
        "1. **Initialize Progress Bar:** Create a progress bar to track the tokenization of images based on the total number of words estimated.\n",
        "2. **Process Files in Batches:** Loop through the file paths in batches, processing a subset of files at a time, up to a defined file limit.\n",
        "3. **Extract Features:** For each batch, extract texts, text features, and layout features using the respective functions.\n",
        "4. **Store Extracted Features:** Append the extracted features to the respective lists for all files.\n",
        "5. **Update Progress Bar:** Update the progress bar based on the number of words processed in each batch.\n",
        "6. **Clear Cache:** Clear the cache and free up memory after processing each batch to ensure efficient memory usage."
      ],
      "metadata": {
        "id": "ZfzrW3xN2shr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUG9Mp0QgMkA"
      },
      "outputs": [],
      "source": [
        "file_count = 0\n",
        "file_limit = 2500\n",
        "with tqdm(total=total_words, desc=\"Tokenizing images\", unit=\"word\", leave=True) as pbar:\n",
        "    for i in range(0, file_limit, batch_size):\n",
        "        # Select the batch of files to process\n",
        "        batch_files = file_paths[i:i+batch_size]\n",
        "\n",
        "        # Process the batch to extract texts\n",
        "        batch_texts = process_batch_1(batch_files)\n",
        "        # Clear memory before proceeding to the next step\n",
        "        clear_memory()\n",
        "\n",
        "        # Process the batch to extract text features\n",
        "        batch_text_features = process_batch_2(batch_files, batch_texts)\n",
        "        text_features.extend(batch_text_features)\n",
        "        # Clear memory before proceeding to the next step\n",
        "        clear_memory()\n",
        "\n",
        "        # Process the batch to extract layout features\n",
        "        batch_layout_features = process_batch_3(batch_files)\n",
        "        # Clear memory before storing results\n",
        "        clear_memory()\n",
        "\n",
        "        # Store the extracted texts along with their file names\n",
        "        texts.extend([(os.path.basename(file_path), text) for file_path, text in zip(batch_files, batch_texts)])\n",
        "\n",
        "        # Store the extracted text and layout features\n",
        "        layout_features.extend(batch_layout_features)\n",
        "\n",
        "        # Clear cache after processing each batch to free up memory\n",
        "        del batch_texts, batch_layout_features, batch_text_features\n",
        "        clear_memory()\n",
        "\n",
        "        # Check if limit is reached\n",
        "        file_count += len(batch_files)\n",
        "        if file_count >= file_limit:\n",
        "            break\n",
        "\n",
        "        # Update the progress bar based on the number of words processed in this batch\n",
        "        pbar.update(len(batch_files) * 512)  # Assuming each file processes approximately 512 words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Maximum Feature Length\n",
        "\n",
        "In this section, we calculate the maximum feature length from the extracted text and layout features. This is important for ensuring that all feature vectors have a consistent length, which is necessary for further processing such as clustering or dimensionality reduction."
      ],
      "metadata": {
        "id": "oLFO-XHo3-n_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2D8j5rXX6Z3"
      },
      "outputs": [],
      "source": [
        "max_length = max([len(np.concatenate((np.squeeze(text_feat), np.squeeze(layout_feat).flatten()))) for text_feat, layout_feat in zip(text_features, layout_features)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine and Normalize Features with Padding or Truncating\n",
        "\n",
        "In this section, we define a function `pad_or_truncate` to ensure that all feature vectors have a consistent length by either padding or truncating them to a fixed length. We then combine the text and layout features, apply the padding/truncating function, and convert the result into a NumPy array for further analysis.\n",
        "\n",
        "The steps involved in this section are:\n",
        "1. **Define Padding/Truncating Function**: Create a function to pad or truncate feature vectors to a specified length.\n",
        "2. **Combine Features**: Concatenate text and layout features for each document.\n",
        "3. **Apply Padding/Truncating**: Use the padding/truncating function to ensure all combined feature vectors have the same length.\n",
        "4. **Store Combined Features**: Append the normalized feature vectors to a list and convert the list to a NumPy array.\n",
        "\n"
      ],
      "metadata": {
        "id": "pZFtD10J4BKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOJepmDsXEv1"
      },
      "outputs": [],
      "source": [
        "# Function to pad or truncate features to a fixed length\n",
        "def pad_or_truncate(feature, length):\n",
        "    if len(feature) > length:\n",
        "        return feature[:length]\n",
        "    elif len(feature) < length:\n",
        "        return np.pad(feature, (0, length - len(feature)), 'constant')\n",
        "    else:\n",
        "        return feature\n",
        "\n",
        "# Combine text and layout features with padding or truncating\n",
        "combined_features = []\n",
        "for text_feat, layout_feat in zip(text_features, layout_features):\n",
        "    text_feat = np.squeeze(text_feat)\n",
        "    layout_feat = np.squeeze(layout_feat).flatten()  # Ensure layout features are flattened\n",
        "    combined_feature = np.concatenate((text_feat, layout_feat))\n",
        "    combined_feature = pad_or_truncate(combined_feature, max_length)\n",
        "    combined_features.append(combined_feature)\n",
        "\n",
        "combined_features = np.array(combined_features)  # Convert to a NumPy array"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking combined-features dimonesions"
      ],
      "metadata": {
        "id": "bIjTjGdK4Upz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Combined features shape: {combined_features.shape}\")\n",
        "print(f\"Combined features size in memory: {combined_features.nbytes / 1e9} GB\")"
      ],
      "metadata": {
        "id": "12YR7wvE4Uza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction using PCA and Kmeans for clustering"
      ],
      "metadata": {
        "id": "h6kWC4qD7NsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA Elbow Plot\n",
        "\n",
        "In this section, we define a function to perform Principal Component Analysis (PCA) on a dataset and plot the cumulative explained variance ratio. The plot helps determine the optimal number of principal components to retain by showing the \"elbow\" point where the explained variance starts to level off."
      ],
      "metadata": {
        "id": "QtKvrCax4V0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_pca_elbow(data, max_components=200):\n",
        "    # Initialize PCA with the maximum number of components, limited by the number of features in the data\n",
        "    pca = PCA(n_components=min(max_components, data.shape[1]))\n",
        "\n",
        "    # Fit the PCA model on the data\n",
        "    pca.fit(data)\n",
        "\n",
        "    # Calculate the cumulative explained variance ratio\n",
        "    explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "    # Plot the cumulative explained variance ratio\n",
        "    plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Variance Explained')\n",
        "    plt.title('Explained Variance by PCA Components')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9yl4QKjvumkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot PCA elbow with reduced dimensionality data\n",
        "print(\"Plotting PCA elbow...\")\n",
        "plot_pca_elbow(combined_features, max_components=1000)"
      ],
      "metadata": {
        "id": "08PU4ryL4gxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying PCA to Meet a Variance Threshold\n",
        "\n",
        "In this section, we define a function to apply Principal Component Analysis (PCA) on a dataset, selecting the number of components required to meet a specified explained variance threshold."
      ],
      "metadata": {
        "id": "x5YbSZV74hmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(data, explained_variance_threshold=0.95):\n",
        "    # Initialize PCA without specifying the number of components\n",
        "    pca = PCA()\n",
        "\n",
        "    # Fit the PCA model on the data\n",
        "    pca.fit(data)\n",
        "\n",
        "    # Calculate the cumulative explained variance ratio\n",
        "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "    # Find the number of components required to meet the explained variance threshold\n",
        "    n_components = np.searchsorted(cumulative_variance, explained_variance_threshold) + 1\n",
        "\n",
        "    # Apply PCA with the estimated number of components\n",
        "    pca = PCA(n_components=n_components)\n",
        "    transformed_data = pca.fit_transform(data)\n",
        "\n",
        "    return transformed_data, n_components\n",
        "\n",
        "transformed_data, n_components = apply_pca(combined_features, explained_variance_threshold=0.95)\n",
        "print(f\"Number of components chosen: {n_components}\")\n"
      ],
      "metadata": {
        "id": "VKb8zkJ9vV9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the Optimal Number of Clusters\n",
        "\n",
        "In this section, we define a function to determine the optimal number of clusters for K-Means clustering using the silhouette score. The silhouette score helps evaluate the quality of the clustering by measuring how similar an object is to its own cluster compared to other clusters."
      ],
      "metadata": {
        "id": "jkOJLo656hPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic8KV_zNYpPk"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def find_optimal_clusters(data, max_k):\n",
        "    # Define the range of cluster numbers to try\n",
        "    iters = range(2, max_k+1, 2)\n",
        "\n",
        "    # Initialize a list to store silhouette scores\n",
        "    s = []\n",
        "\n",
        "    # Iterate through the range of cluster numbers\n",
        "    for k in iters:\n",
        "        # Perform K-Means clustering with k clusters\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
        "\n",
        "        # Calculate the silhouette score for the current clustering\n",
        "        s.append(silhouette_score(data, kmeans.labels_))\n",
        "\n",
        "        # Print the number of clusters and corresponding silhouette score\n",
        "        print(f'k: {k}, Silhouette Score: {s[-1]}')\n",
        "\n",
        "    # Create a plot to visualize the silhouette scores for different cluster numbers\n",
        "    f, ax = plt.subplots(1, 1)\n",
        "    ax.plot(iters, s, marker='o')\n",
        "    ax.set_xlabel('Cluster Centers')\n",
        "    ax.set_xticks(iters)\n",
        "    ax.set_xticklabels(iters)\n",
        "    ax.set_ylabel('Silhouette Score')\n",
        "    ax.set_title('Silhouette Scores for Various Clusters')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Clustering with the Optimal Number of Clusters\n",
        "\n",
        "In this section, we use the optimal number of clusters identified from the silhouette analysis to perform K-Means clustering."
      ],
      "metadata": {
        "id": "S57FBtf16v4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7z2qLALYtHp"
      },
      "outputs": [],
      "source": [
        "# Perform clustering with the optimal number of clusters\n",
        "optimal_clusters = 4  # Adjust this based on the silhouette analysis\n",
        "print(f\"Clustering into {optimal_clusters} clusters...\")\n",
        "\n",
        "# Initialize K-Means with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "\n",
        "# Fit the K-Means model and predict the cluster for each data point\n",
        "clusters = kmeans.fit_predict(transformed_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Clustering with Multiple Metrics\n",
        "\n",
        "In this section, we evaluate the quality of the clustering using three different metrics: Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index. These metrics provide a comprehensive assessment of the clustering performance."
      ],
      "metadata": {
        "id": "zhlIcFsm7EaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wio2E7elZGcu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "# Evaluate the clustering using the three metrics\n",
        "silhouette_avg = silhouette_score(transformed_data, clusters)\n",
        "davies_bouldin_avg = davies_bouldin_score(transformed_data, clusters)\n",
        "calinski_harabasz_avg = calinski_harabasz_score(transformed_data, clusters)\n",
        "\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin_avg}\")\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz_avg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction using t-SNE and Kmeans for clustering"
      ],
      "metadata": {
        "id": "XpDJ5dDk7lr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply t-SNE for Dimensionality Reduction\n",
        "\n",
        "In this section, we define a function to apply t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction. t-SNE is a powerful technique for visualizing high-dimensional data by mapping it to a lower-dimensional space, typically 2 or 3 dimensions."
      ],
      "metadata": {
        "id": "whgqxeG_7yJM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S43BH89Z-8c"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def apply_tsne(data, n_components=2, perplexity=30.0, n_iter=1000):\n",
        "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=n_iter)\n",
        "    transformed_data = tsne.fit_transform(data)\n",
        "    return transformed_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = 30  # Perplexity value\n",
        "n_iter = 1000  # Number of iterations\n",
        "transformed_data = apply_tsne(combined_features, n_components=2, perplexity=perplexity, n_iter=n_iter)"
      ],
      "metadata": {
        "id": "KjpwAdoTyIyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot t-SNE Results\n",
        "\n",
        "In this section, we define a function to apply t-distributed Stochastic Neighbor Embedding (t-SNE) for dimensionality reduction and plot the resulting lower-dimensional data. This visualization helps in understanding the structure and distribution of high-dimensional data in a 2D space."
      ],
      "metadata": {
        "id": "LIAtIWsV8G2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot t-SNE results\n",
        "def plot_tsne(data, perplexity, n_iter):\n",
        "    \"\"\"\n",
        "    Apply t-SNE and plot the results.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The input data to be transformed.\n",
        "    - perplexity: The perplexity parameter for t-SNE.\n",
        "    - n_iter: The number of iterations for optimization.\n",
        "    \"\"\"\n",
        "    # Initialize t-SNE with the specified parameters\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter)\n",
        "\n",
        "    # Fit and transform the data using t-SNE\n",
        "    transformed_data = tsne.fit_transform(data)\n",
        "\n",
        "    # Plot the transformed data\n",
        "    plt.scatter(transformed_data[:, 0], transformed_data[:, 1], s=5)\n",
        "    plt.title(f't-SNE Visualization (perplexity={perplexity}, n_iter={n_iter})')\n",
        "    plt.xlabel('t-SNE Component 1')\n",
        "    plt.ylabel('t-SNE Component 2')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example data\n",
        "data = np.random.rand(1000, 100)\n",
        "\n",
        "# Different parameter settings\n",
        "plot_tsne(combined_features, perplexity=5, n_iter=300)\n",
        "plot_tsne(combined_features, perplexity=30, n_iter=300)\n",
        "plot_tsne(combined_features, perplexity=50, n_iter=300)\n",
        "plot_tsne(combined_features, perplexity=30, n_iter=1000)\n"
      ],
      "metadata": {
        "id": "osXN1T8myKh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the optimal clusters\n",
        "\n",
        "Finding the optimal clusters using the function defined before"
      ],
      "metadata": {
        "id": "3CFTN_NO8qQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Finding the optimal number of clusters...\")\n",
        "find_optimal_clusters(transformed_data, 10)"
      ],
      "metadata": {
        "id": "TtdZ50pmyMn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Clustering with the Optimal Number of Clusters\n",
        "\n",
        "In this section, we use the optimal number of clusters identified from the silhouette analysis to perform K-Means clustering."
      ],
      "metadata": {
        "id": "u-rY6LNB9H-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform clustering with the optimal number of clusters\n",
        "optimal_clusters = 4  # Adjust this based on the silhouette analysis\n",
        "print(f\"Clustering into {optimal_clusters} clusters...\")\n",
        "\n",
        "# Initialize K-Means with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "\n",
        "# Fit the K-Means model and predict the cluster for each data point\n",
        "clusters = kmeans.fit_predict(transformed_data)\n"
      ],
      "metadata": {
        "id": "mBApcBllyOLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Clustering with Multiple Metrics\n",
        "\n",
        "In this section, we evaluate the quality of the clustering using three different metrics: Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index. These metrics provide a comprehensive assessment of the clustering performance."
      ],
      "metadata": {
        "id": "gKR-iehU9PML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the clustering using the three metrics\n",
        "silhouette_avg = silhouette_score(transformed_data, clusters)\n",
        "davies_bouldin_avg = davies_bouldin_score(transformed_data, clusters)\n",
        "calinski_harabasz_avg = calinski_harabasz_score(transformed_data, clusters)\n",
        "\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "print(f\"Davies-Bouldin Index: {davies_bouldin_avg}\")\n",
        "print(f\"Calinski-Harabasz Index: {calinski_harabasz_avg}\")\n"
      ],
      "metadata": {
        "id": "U9U56Db7yQQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the clustring with best metrics"
      ],
      "metadata": {
        "id": "Zu-wQz9t9QfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories for each cluster\n",
        "import shutil\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "for cluster_label in np.unique(kmeans_labels):\n",
        "    cluster_dir = os.path.join(output_folder_path, f\"cluster_{cluster_label}\")\n",
        "    os.makedirs(cluster_dir, exist_ok=True)\n",
        "\n",
        "# Move images to the corresponding cluster directories\n",
        "for image_path, cluster_label in zip(file_paths, kmeans_labels):\n",
        "    shutil.copy(image_path, os.path.join(output_folder_path, f\"cluster_{cluster_label}\"))\n",
        "\n",
        "print(\"Images have been saved to corresponding cluster directories.\")"
      ],
      "metadata": {
        "id": "T2VJf0qxy90R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}